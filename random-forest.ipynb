{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)Fit Random Forest models using each possible input on its own to predict edibility. Evaluate the quality of fit by using the predict function to calculate the predicted class for each mushroom (edible or poisonous). Which input fits best? (i.e. which classifies the most mushrooms correctly?) (0.5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of CapShape model: 0.564\n",
      "Accuracy of CapSurface model: 0.581\n",
      "Accuracy of CapColor model: 0.595\n",
      "Accuracy of Odor model: 0.985\n",
      "Accuracy of Height model: 0.518\n",
      "\n",
      "Best feature: Odor (accuracy: 0.985)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(r\"C:\\Users\\hughd\\Desktop\\machine learning\\mushrooms.csv\")\n",
    "\n",
    "# Split the data into input and output variables\n",
    "X = data[['CapShape', 'CapSurface', 'CapColor', 'Odor', 'Height']].copy()\n",
    "y = data['Edible']\n",
    "\n",
    "# Encode the categorical variables as numerical values\n",
    "le = LabelEncoder()\n",
    "for col in X.columns:\n",
    "    X[col] = le.fit_transform(X[col])\n",
    "\n",
    "# Fit Random Forest models using each input variable on its own\n",
    "best_acc = 0\n",
    "best_feature = ''\n",
    "for feature in X.columns:\n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(X[[feature]], y)\n",
    "    acc = (rf.predict(X[[feature]]) == y).mean()\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_feature = feature\n",
    "    print(f\"Accuracy of {feature} model: {acc:.3f}\")\n",
    "\n",
    "# Print the best input variable\n",
    "print(f\"\\nBest feature: {best_feature} (accuracy: {best_acc:.3f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Random Forest Model odor is by far the most accurate catergory being 98.5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Using cross-validation, perform a model selection to determine which features are useful for making predictions using a Random Forest. As above, use the number of mushrooms correctly classified as the criterion for deciding which model is best. You might try to find a way to loop over all 32 possible models. Or select features ‘greedily’, by picking one at a time to add to the model. Present your results in the most convincing way you can. (2 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best 1feature: ['Odor'] (accuracy: 0.984)\n",
      "Best 2feature: ['CapColor', 'Odor'] (accuracy: 0.987)\n",
      "Best 3feature: ['CapShape', 'CapColor', 'Odor'] (accuracy: 0.991)\n",
      "Best 4feature: ['CapShape', 'CapSurface', 'CapColor', 'Odor'] (accuracy: 0.991)\n",
      "Best 5feature: ['CapShape', 'CapSurface', 'CapColor', 'Odor', 'Height'] (accuracy: 0.990)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(r\"C:\\Users\\hughd\\Desktop\\machine learning\\mushrooms.csv\")\n",
    "\n",
    "# Split the data into input and output variables\n",
    "X = data[['CapShape', 'CapSurface', 'CapColor', 'Odor', 'Height']].copy()\n",
    "y = data['Edible']\n",
    "\n",
    "# Encode the categorical variables as numerical values\n",
    "le = LabelEncoder()\n",
    "for col in X.columns:\n",
    "    X.loc[:, col] = le.fit_transform(X.loc[:, col])\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Fit Random Forest models using each combination of input variables\n",
    "best_acc = [0]*5\n",
    "best_feature_sets = [[] for i in range(5)]\n",
    "for i in range(1, 6):\n",
    "    for feature_subset in itertools.combinations(X.columns, i):\n",
    "        rf = RandomForestClassifier()\n",
    "        rf.fit(X_train[list(feature_subset)], y_train)\n",
    "        acc = (rf.predict(X_test[list(feature_subset)]) == y_test).mean()\n",
    "        if acc > best_acc[i-1]:\n",
    "            best_acc[i-1] = acc\n",
    "            best_feature_sets[i-1] = list(feature_subset)\n",
    "\n",
    "# Print the best input variables for each subset size\n",
    "for i in range(1, 6):\n",
    "    print(f\"Best {i}feature: {best_feature_sets[i-1]} (accuracy: {best_acc[i-1]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I used a greedy method for choosing the classifacation and found that odor on its own is still very accurate. As more catagories are added they accuracy is slowly increased until the final. After running this a few times it seems that height is the least important factor decreasing the accuracy of the 5th set. Odor becomes less important as more and more factors are added, but standalone it is significantly the best. The set of properties including the attributes of the cap become very accurate when all three are taken into consideration and become the top three important factors in set 4 and 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Would you use this classifier if you were foraging for mushrooms? Discuss with reference to factors that you identified as important and the probability of poisoning yourself. (0.5 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For me I feel that i would stick primarily to odor alone as the accuracy only changes slightly when more and more factors added. It is much simpler and easy for Humans to learn by just odor and it has a very similar accuracy to using all of the factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Fit an ANN module using each possible input on its own to predict edibility. Evaluate the quality of fit by using the predict function to calculate the predicted class for each mushroom (edible or poisonous). Which input fits best? (i.e. which classifies the most mushrooms correctly?) (0.5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ANN for column: CapShape\n",
      "Accuracy for column CapShape: 0.5612307692307692\n",
      "Training ANN for column: CapSurface\n",
      "Accuracy for column CapSurface: 0.5784615384615385\n",
      "Training ANN for column: CapColor\n",
      "Accuracy for column CapColor: 0.5969230769230769\n",
      "Training ANN for column: Odor\n",
      "Accuracy for column Odor: 0.9846153846153847\n",
      "Training ANN for column: Height\n",
      "Accuracy for column Height: 0.5187692307692308\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Separate the target variable and input features\n",
    "X = data.drop(columns=[\"Edible\"])\n",
    "y = data[\"Edible\"]\n",
    "\n",
    "# Train an ANN for each column separately and evaluate its performance\n",
    "for col in X.columns:\n",
    "    print(f\"Training ANN for column: {col}\")\n",
    "   \n",
    "    # Create a new column with one-hot encoded categorical data\n",
    "    if X[col].dtype == \"object\":\n",
    "        X_one_hot = pd.get_dummies(X[col], prefix=col)\n",
    "        X_col = X_one_hot.values\n",
    "    else:\n",
    "        X_col = X[[col]].values\n",
    "   \n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_col, y, test_size=0.2, random_state=42)\n",
    "   \n",
    "    # Create MLP classifier with one hidden layer of 5 neurons\n",
    "    model = MLPClassifier(hidden_layer_sizes=(5,), max_iter=500)\n",
    "   \n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "   \n",
    "    # Evaluate model on test data\n",
    "    score = model.score(X_test, y_test)\n",
    "    print(f\"Accuracy for column {col}: {score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results for this method are very similar to when the random forest ran the same question. Similarly, the odor was significantly the best on its own having nearly double the accuracy than that of any other factor alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Explore how the performance depends on the architecture of the ANN. Vary the number and the sizes of the hidden layers. For large networks you may want to increase the number of the stochastic gradient descent iterations. (1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for column CapShape, hidden layer sizes (5,): 0.556923076923077\n",
      "Accuracy for column CapSurface, hidden layer sizes (5,): 0.5476923076923077\n",
      "Accuracy for column CapColor, hidden layer sizes (5,): 0.5723076923076923\n",
      "Accuracy for column Odor, hidden layer sizes (5,): 0.984\n",
      "Accuracy for column Height, hidden layer sizes (5,): 0.5144615384615384\n"
     ]
    }
   ],
   "source": [
    "# Set up parameters for the ANNs\n",
    "num_iter = 1000# Increase for large networks\n",
    "hidden_layer_sizes = [(5,)]  # Vary number and sizes of hidden layers\n",
    "\n",
    "# Train an ANN for each column separately and evaluate its performance\n",
    "for col in X.columns:\n",
    "    #print(f\"Training ANNs for column: {col}\")\n",
    "   \n",
    "    # Create a new column with one-hot encoded categorical data\n",
    "    if X[col].dtype == \"object\":\n",
    "        X_one_hot = pd.get_dummies(X[col], prefix=col)\n",
    "        X_col = X_one_hot.values\n",
    "    else:\n",
    "        X_col = X[[col]].values\n",
    "   \n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_col, y, test_size=0.2)\n",
    "   \n",
    "    # Train and evaluate ANNs with different architectures\n",
    "    for hidden_sizes in hidden_layer_sizes:\n",
    "        #print(f\"Training ANN with hidden layer sizes: {hidden_sizes}\")\n",
    "       \n",
    "        # Create MLP classifier with specified hidden layer sizes\n",
    "        model = MLPClassifier(hidden_layer_sizes=hidden_sizes, max_iter=num_iter)\n",
    "       \n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "       \n",
    "        # Evaluate model on test data\n",
    "        score = model.score(X_test, y_test)\n",
    "        print(f\"Accuracy for column {col}, hidden layer sizes {hidden_sizes}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After exploring diffrent hidden layers I found that some factors are more heavily infuenced by the hidden layers than others specifically cap colour that had more variation than that of cap shape which had none. As this is a large network the program struggled to run at below 500 iterations and it plataues at approximately 700"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Using cross-validation, perform a model selection to determine which features are useful for making predictions using the ANN. As above, use the number of mushrooms correctly classified as the criterion for deciding which model is best. You might try to find a way to loop over all 32 possible models. Or select features ‘greedily’, by picking one at a time to add to the model. Present your results in the most convincing way you can. (2 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best 1-feature subset: ['Odor'] (accuracy: 0.600)\n",
      "Best 2-feature subset: ['Odor', 'CapShape'] (accuracy: 0.986)\n",
      "Best 3-feature subset: ['Odor', 'CapShape', 'CapSurface'] (accuracy: 0.988)\n",
      "Best 4-feature subset: ['Odor', 'CapShape', 'CapSurface', 'Height'] (accuracy: 0.987)\n",
      "Best 5-feature subset: ['Odor', 'CapShape', 'CapSurface', 'Height', 'CapColor'] (accuracy: 0.772)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Convert categorical data to numerical values\n",
    "encoder = LabelEncoder()\n",
    "for col in data.columns:\n",
    "    data[col] = encoder.fit_transform(data[col])\n",
    "\n",
    "# Separate the features and target\n",
    "X = data.drop('Edible', axis=1)\n",
    "y = data['Edible']\n",
    "\n",
    "# Initialize an empty list to store the selected features\n",
    "select_feat = []\n",
    "\n",
    "# Loop over the features and select the one that gives the highest accuracy score\n",
    "for i in range(len(X.columns)):\n",
    "    best_feature, best_score = None, 0\n",
    "    for feat in X.columns:\n",
    "        if feat not in select_feat:\n",
    "            # Combine the previously selected features with the current feature\n",
    "            feats = select_feat + [feat]\n",
    "            # Split the data into training and validation sets\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X[feats], y, test_size=0.3)\n",
    "            # Train an ANN on the training set\n",
    "            model = MLPClassifier(hidden_layer_sizes=(5,), max_iter=3000)\n",
    "            model.fit(X_train, y_train)\n",
    "            # Predict the classes of the validation set\n",
    "            y_pred = model.predict(X_val)\n",
    "            # Compute the accuracy score\n",
    "            score = accuracy_score(y_val, y_pred)\n",
    "            # Update the best feature and score if this one is better\n",
    "            if score > best_score:\n",
    "                best_feature, best_score = feat, score\n",
    "    # Add the best feature to the selected features list\n",
    "    select_feat.append(best_feature)\n",
    "    # Print the current subset and its accuracy score\n",
    "    print(f\"Best {i+1}-feature subset: {select_feat} (accuracy: {best_score:.3f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I used a greedy method for choosing the classifacation and found that ,similar to random forest, odor on its own is still the most accurate, however ANN thinks its much less accurate. As more catagories are added they accuracy is slowly increased until the final. After running this a few times it seems that cap colour is the least important factor decreasing the accuracy significantly on the 5th set. Odor stays the most important as more and more factors are added. The final accuracy is ranked lower than that of random forest model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Compare the performance of Random Forest and ANN models. For example, which data types, do you think, the two ML models are most suited to describe. (0.5 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think that ANN results are likely to be more accurate as it is more suited to non numerical functions simulating neural pathways. Random forest is great for numerical representations, but I feel less accurate in this case. Overall for this particular discription I would trust ANN model more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
